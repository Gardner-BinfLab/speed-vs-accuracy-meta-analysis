\documentclass[fleqn,10pt]{SelfArx} % Document font size and equations flushed left

\usepackage{float}
\usepackage{graphicx}
\usepackage{caption}
\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{stmaryrd}
\usepackage{todonotes}
\usepackage{maori}
\graphicspath{{../figures/}}
\setlength{\abovecaptionskip}{15pt plus 3pt minus 2pt}

\newcommand{\beginsupplement}{%
        \setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}%
        \setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}%
     }

\definecolor{color1}{RGB}{0,0,90} % Color of the article title and sections
\definecolor{color2}{RGB}{200,200,200} % Color of the boxes behind the abstract and headings
\definecolor{color3}{RGB}{5,5,5} % Color of the boxes behind the abstract and headings

\JournalInfo{.} % Journal information ``Journal, Vol. XXI, No. 1, 1-5, 2015''
\Archive{ } % Additional notes (e.g. copyright, DOI, review/research article)

\def\numTools{499}
%cat meanRankSpeedData.tsv | cut -f 4 | grep -v ^method$ | wc -l
\def\numBenchmarkPubs{69}
%cat speed-vs-accuracy-toolRanks2005-2020.tsv | cut -f 1 | sort  | uniq | grep -E '[0-9]'  | wc -l
\def\numBenchmarks{134}
%cat rawRankSpeedData2005-2020.tsv | cut -f 1 | grep -v ^testId$ | uniq -c | wc -l

\PaperTitle{A meta-analysis of bioinformatics software benchmarks reveals that publication-bias unduly influences software accuracy} % Article title

\Authors{Paul P. Gardner\textsuperscript{1,2}*, James M. Paterson\textsuperscript{3}, Stephanie McGimpsey\textsuperscript{4}, Fatemeh Ashari-Ghomi\textsuperscript{5}, Sinan U. Umu\textsuperscript{6}, Aleksandra Pawlik\textsuperscript{7},
Alex Gavryushkin\textsuperscript{8}, Michael A Black\textsuperscript{1}
} % Authors

\affiliation{\textsuperscript{1}\textit{Department of Biochemistry, University of Otago, Dunedin, New Zealand.}} % Author affiliation
\affiliation{\textsuperscript{2}\textit{Biomolecular Interaction Centre, University of Canterbury, Christchurch, New Zealand.}}
\affiliation{\textsuperscript{3}\textit{Department of Civil and Natural Resources Engineering, University of Canterbury, Christchurch, New Zealand.}}
\affiliation{\textsuperscript{4}\textit{Parasites and Microbes, Wellcome Sanger Institute, Wellcome Genome Campus, Hinxton,
8 Cambridgeshire, CB10 1RQ, UK.}}
\affiliation{\textsuperscript{5}\textit{Research Group for Genomic Epidemiology, National Food Institute, Technical University of Denmark, Kongens Lyngby, Denmark.}}
\affiliation{\textsuperscript{6}\textit{Department of Research, Cancer Registry of Norway, Oslo, Norway.}} % Author affiliation
\affiliation{\textsuperscript{7}\textit{New Zealand eScience Infrastructure, 49 Symonds St, Auckland, New Zealand.}}
\affiliation{\textsuperscript{8}\textit{Department of Computer Science, University of Otago, Dunedin, New Zealand.}} % Author affiliation

\affiliation{*\textbf{Corresponding author}: paul.gardner@otago.ac.nz} % Corresponding author

\Keywords{} % Keywords - if you don't want any simply remove all the text between the curly brackets
\newcommand{\keywordname}{Keywords} % Defines the keywords heading name

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

%\Abstract{In the below we provide additional results for our investigation of computational biology benchmarks.}
\Abstract{ Computational biology has provided widely used and powerful
  software tools for testing and making inferences about biological
  data. In the face of rapidly increasing volumes of data, heuristic
  methods that trade software speed for accuracy must
  be employed.  We are interested in testing for trade-offs between
  speed and accuracy and for other factors that are indicative of accurate
  software.

We have extracted accuracy and speed ranks from independent benchmarks of
computational biology software of different software tools, and evaluated the
factors that are likely to influence accuracy e.g. speed, author
reputation, journal impact, recency or developer efforts.

We found that software speed, author reputation, journal impact, the
number of citations and age are all unreliable predictors of software
accuracy. This is unfortunate because citations, author and journal
reputation are frequently used reasons for selecting software
tools. Our results do show that accurate bioinformatic software tools
are generally the product of many gradual improvements, often from
multiple developers, based upon matching GitHub records with benchmark results. 
In addition, we find
that the field of bioinformatics has an excess of slow and inaccurate software
tools, this is consistent across many sub-disciplines. Meanwhile, there are few tools that
are middle-of-road in terms of accuracy and speed trade-offs. We
hypothesise that a form of publication-bias unduly influences the
publication and development of bioinformatic software. In other
words, software that is intermediate in terms of both speed and accuracy may be difficult to publish due to author,
editor and reviewer practices. This leaves an unfortunate gap in the
literature as the ideal tools may fall in the gap e.g. high accuracy tools are not always useful if years of CPU time are required, 
while high speed is not useful if the results are also inaccurate.  }

\begin{document}

\flushbottom % Makes all text pages the same height
\maketitle % Print the title and abstract box
%\tableofcontents % Print the contents section

\thispagestyle{empty} % Removes page numbering from the first page

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

%\onecolumn
%\beginsupplement



%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------



%%REFERENCES::
%reference Stefan Buchka$^{1}$, Rory Wilson$^2$, Alexander Hapfelmeier$^3$,\\  Paul Gardner$^4$,  Anne-Laure Boulesteix PAPER!!!

%% A comprehensive analysis of the usability and archival stability of omics computational tools and resources
%% https://www.biorxiv.org/content/10.1101/452532v1

%% Tools and best practices for data processing in allelic expression analysis
%% https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0762-6

%% Towards reproducible, transparent, and systematic benchmarking of omics computational tools
%% https://osf.io/p8yd9

%% Essential guidelines for computational method benchmarking
%% https://arxiv.org/abs/1812.00661

\section*{Background}
Computational biology software is widely used and has produced some of
the most cited publications in the entire scientific corpus
\cite{Perez-Iratxeta2007-lv,Van_Noorden2014-kc,Wren2016-xy}. This
highly-cited software includes implementations of methods for sequence
alignment and homology inference
\cite{Altschul1990-ht,Thompson1994-eu,Thompson1997-rl,Altschul1997-ga},
phylogenetic analysis
\cite{Felsenstein1985-lj,Saitou1987-zl,Posada1998-qq,Ronquist2003-yh,Tamura2007-ei},
statistical analysis of survival patterns in biomedicine
\cite{Kaplan1958-ju,Cox1972-nu}, biomolecular structure analysis
\cite{Sheldrick1990-kc,Sheldrick2008-xy,Jones1991-ik,Laskowski1993-vi,Otwinowski1997-xj},
visualization and data collection
\cite{Kraulis1991-lt,Berman2000-to}. However, the popularity of a
software tool does not necessarily mean that it is accurate or
computationally efficient, instead usability, ease of installation,
operating system and other factors may play a greater role in a software
tool's popularity. Indeed, there have been several notable incidences where
inaccurate software has caused considerable harm
\cite{leveson1993investigation,cummings2020regulating,Herkert:2020}.
%Fergusson covid predictions? 

Progress in the biological sciences is increasingly limited by the
ability to analyse large volumes of data, therefore the
dependence of biologists on software is also increasing
\cite{Marx2013-zi}. There is an increasing reliance on technological
solutions for automating biological data generation
(e.g. next-generation sequencing, mass-spectroscopy, cell-tracking and
species tracking), therefore the biological sciences have become
increasingly dependent upon computational software for processing
large quantities of data \cite{Marx2013-zi}. As a consequence, the
computational efficiency of data processing and analysis software is
of great importance to decrease the energy, climate impact, and time costs of research
\cite{Gombiner2011-md}. Furthermore, even small error rates can have
major impacts on the number of false inferences as datasets become
larger \cite{Storey2003-cv}.

The gold-standard for determining accuracy is for independent
researchers to conduct benchmarks, which can serve a useful role in
reducing the over-optimistic reporting of software accuracy
\cite{Boulesteix2010-te,Jelizarow2010-zf,Weber:2019} and the self-assessment trap
\cite{Norel2011-cq,Buchka:2021}. Benchmark studies typically use a number of
positive and negative control datasets, predictions from different software tools can then be
partitioned into true or false groups and a variety of metrics can be
used to evaluate the performance differences
\cite{Egan1975-nd,Hall2012-kg,Weber:2019}.
%Some benchmarks now use live, or
%frequently updated, results to indicate the latest developments in
%software performance
%\cite{Bujnicki2001-xr,Puton2014-hy,Barton_undated-er}.
The aim of these benchmarks is to independently identify tools that
make acceptable compromises in terms of balancing speed with
discriminating true and false predictions, and are therefore suited
for wide adoption by the community.

For common computational biology tasks, a proliferation of
software-based solutions often exists
\cite{Felsenstein1995-ic,Altschul2013-bv,Henry2014-ut}. While
this is a good problem to have, and points to a
diversity of options from which practical solutions can be selected,
having many possible options creates a dilemma for users. In the absence of
any recent gold-standard benchmarks, how should scientific software be
selected? In the following we presume that ``biological
accuracy'' is the most desirable feature for a software tool. Biological
accuracy is the degree to which predictions or measurements reflect
the biological truths based on expert-derived curated datasets (see Methods for the  mathematical definition used here). In some
fields biological accuracy is very difficult to ascertain, for
example, in phylogenetics it is nearly impossible in most cases to know the
true ancestral relationships between organisms. In situations like this,
researchers may use simulated or high-confidence datasets.

A number of possible predictors of software quality are used by the
community of computational biology software users \cite{Hannay2009-cf,Joppa2013-vj,Loman2015-bw}. Some accessible,
quantifiable and frequently used proxies for identifying high quality
software include: \textbf{1. Recency:} recently published software
tools may have built upon the results of past work, or be an update to
an existing software. Therefore these may be more accurate and
faster. \textbf{2. Wide adoption:} a software tool may be widely used
because it is fast and accurate, or because it is well-supported and
user-friendly. In fact, “large user base”, “word-of-mouth”,
“wide-adoption”, "personal recommendation," and "recommendation from a
close colleague," are frequent responses to surveys of “how do
scientists select software?”
\cite{Hannay2009-cf,Joppa2013-vj,Loman2015-bw}. \textbf{3. Journal
  impact:} high profile journals are run by editors and reviewers who
carefully select and curate the best manuscripts. Therefore, high
impact journals may be more likely to select manuscripts describing
good software \cite{Garfield1955-wf}. \textbf{4. Author/Group
  reputation:} the key to any project is the skills of the people
involved, including maintaining a high collective intelligence
\cite{Joppa2013-vj,Woolley2010-ld,Cheruvelil2014-xn}. As a
consequence, an argument could be made that well respected and
high-profile authors will produce better software
\cite{Hirsch2005-mt,Bornmann2008-il}. \textbf{5. Speed:} software tools
frequently trade accuracy for speed. For example, heuristic
software such as the popular homology search tool, BLAST, compromises
the mathematical guarantee of optimal solutions for more speed
\cite{Altschul1990-ht,Altschul1997-ga}. Some researchers may naively
interpret this fact as slower software is likely to be more
accurate. But speed may also be influenced by the programming language
\cite{Fourment2008-vl}, and the level of hardware optimisation
\cite{Farrar2007-ky,Dematte2010-ph};
however, the implementation generally has more of an impact (e.g. brute-force approaches versus
rapid and sensitive pre-filtering
\cite{Schaeffer1989-mu,Papadimitriou_undated-bo}).
%\todo{This paper
%https://science.sciencemag.org/content/368/6495/eaam9744.full
%shows a thousands-fold increase in speed when parallelisation
%is implemented with things like cache, architecture, etc.
%mind VS naive parallelisation as in Python, R, etc.}

Other factors that are less quantifiable that influence whether a
software tool is selected include: whether the documentation is good,
user-friendly, word-of-mouth and ``used in a similar analysis''
\cite{Loman2015-bw}. However, citation metrics may be a
useful proxy for the above.
%This may also explain the reason why
%some software continues to be used, in spite of poor relative
%performance \cite{Wadi2016-dj}.
%We note that none of these factors
%are directly linked to the top ranked features of successful software
%projects, as based upon literature surveys \cite{nasir2011critical}.

With the wide adoption of GitHub ($\approx 47\%$ of the
{\color{black}\numTools} tools included in this study could be
linked to a GitHub repository), and consequently quantifiable data on software
development such as the number of contributors to code, number of code
changes and versions is also available
\cite{ray2014large,Dozmorov:2018,mangul2018comprehensive}.

In the following study, we explore factors that may be indicative of
software accuracy. This, in our opinion, should be one of the prime
reasons for selecting a software tool. We have mined the large and
freely accessible PubMed database \cite{Sayers2010-vm} for benchmarks
of computational biology software, and manually extracted accuracy and
speed rankings for {\color{black}\numTools~} unique software tools. For
each tool, we have collected measures that may be predictive
of accuracy, and may be subjectively employed by the research
community as a proxy for software quality. These include relative
speed, relative age, the productivity and impact of the corresponding
authors, journal impact and the number of citations.

\section*{Results}
We have collected relative accuracy and speed ranks for
{\color{black}\numTools~} distinct software tools. This software has
been developed for solving broad cross-section computational biology
tasks.
Each software tool was benchmarked in at least one of
{\color{black}\numBenchmarkPubs~} publications that satisfy the Boulesteix
criteria \cite{Boulesteix2013-vb}. In brief, the Boulesteix criteria
are: 1. the main focus of the article is a benchmark. 2. the authors
are reasonably neutral. 3. the test data and evaluation criteria are
sensible.


\begin{figure*}[htb!]
\includegraphics[width=\textwidth]{figure1.pdf}
\caption{\textbf{A.} A heatmap indicating the relationships between
  proposed predictors of software quality. Spearman’s rho is used to
  infer correlations between metrics such as the H- and M-indices of
  corresponding authors, number of citations, journal impact factors
  and H5 indices, the year and relative age of software and the mean
  relative rankings of software speed and accuracy. Red colours
  indicate a positive correlation, blue colours indicate a negative
  correlation. Correlations with an uncorrected P-value less than 0.05 are
  indicated with a `$\vartimes$'. The dashed rectangular area is illustrated in
  more detail in \textbf{B}, the relationship between speed and
  accuracy is shown in more detail in \textbf{Figure 2}.
  %The
  %dendrogram was computed using the default ‘heatmap.2 function in R,
  %which computes a Euclidean distance matrix and a complete-linkage
  %hierarchical clustering.
  \textbf{B.} A barplot illustrating the correlation, as measured by
  Spearman’s rho, between normalised accuracy ranks and software tool
  features that may be predictive of accuracy. In order to give an
  appreciation of the difference between the observed effect-sizes and
  significant effect sizes, we generated 1000 permuted accuracy ranks
  for each benchmark and recorded Spearman’s rho for the significant
  correlations with an uncorrected P-value less than 0.05.
  These values are marked with blues `$\vartimes$'s in the
  barplot. Significant correlations are marked with red asterisks.}
\label{fig:allfactors}
\end{figure*}


For each of the publications describing these tools, we have (where
possible) collected the journal's H5-index (\href{https://scholar.google.co.nz/citations?view\_op=top\_venues\&hl=en}{Google
  Scholar Metrics}), the maximum H-index and
corresponding M-indices \cite{Hirsch2005-mt} for the corresponding
authors for each tool, and the number of times the publication(s)
associated with a tool has been cited using Google Scholar (data
collected over a 6 month period in late 2020). Note that citation metrics
are not static and will change over time. In addition, where possible we also extract the version number, the number
of commits and number of contributors from GitHub repositories.  

We have computed the Spearman’s correlation coefficient for each
pairwise combination of the mean normalised accuracy and speed ranks,
with the year published, mean relative age (compared to software in the
same benchmarks), journal H5 metrics, the total number of citations,
the relative number of citations (compared to software in the same
benchmarks) and the maximum H- and corresponding M-indices for the corresponding
authors, version number, and numbers of commits and contributors.
The results are presented in Figure~\ref{fig:allfactors}A. We find
significant associations between most of the citation-based metrics
(journal H5, citations, relative citations, H-index and
M-index). There is also a negative correlation between the year of
publication, the relative age and many of the citation-based metrics.

Data on the
number of updates to software tools from GitHub such as the number of versions,
commits and contributors was significantly correlated with 
accuracy (respective Spearman’s rhos = {\color{black}0.13, 0.22, 0.21,} respective uncorrected P-values = {\color{black}0.0027, 0.00079, 0.0013}). 
These features were not correlated with speed however (see Figure~\ref{fig:allfactors}A).
We also found that author reputation metrics, journal impacts and the age
of tools were generally \textbf{not} correlated with either tool
accuracy or speed (see Figure~\ref{fig:allfactors}). The strongest
association was between accuracy and the relative
number of citations (Spearman’s rho = {\color{black}0.092,
P-value = 0.047}). But the effect size is only just over the
significance threshold and may be the result of multiple-testing. 

%To further validate this result, we compute a correlation
%between speed and accuracy for each benchmark and used weighted sum
%Z-tests \cite{Zaykin2011-tj}. This also failed to identify a
%significant relationship (sum Z=-0.1, P-value=0.5).

In order to gain a deeper understanding of the distribution of
available bioinformatic software tools on a speed versus accuracy
landscape, we ran a permutation test. The ranks extracted
from each benchmark were randomly permuted, generating 1,000
randomized speed and accuracy ranks. In the cells of a $10\times10$
grid spanning the normalised speed and accuracy ranks we computed a
Z-score for the observed number of tools in a cell, compared to the
expected distributions generated by 1,000 randomized ranks. The results
of this are shown in Figure~\ref{fig:speedaccuracy}. We identified {\color{black}18}
bins where there was a significant excess or dearth of tools. For
example, there was an excess of “slow and inaccurate” software ({\color{black}Z=2.50,
P-value=0.0063}). We find that the amount of software classed as “fast
and accurate”, “fast and inaccurate” and "slow and accurate" are at approximately the
expected proportions based upon the permutation test. 
While there are very few tools placed in the middle of the ranks on accuracy and speed.
The number of
significant results is in excess, and is not due to multiple testing, as
the probability of finding {\color{black}18} of 100 tests significant by chance is
low (P-value = {\color{black}$2.2\times 10^{-6}$}, exact binomial test).

% binom.test(sigCount,gridRes^2, p = 0.05)

\begin{figure*}[htb!]
%\includegraphics[scale=0.55]{figure2.pdf}
\centering
\includegraphics[width=0.75\textwidth]{figure2.pdf}
\caption{A heatmap indicating the relative paucity or abundance of
  software in the range of possible accuracy and speed rankings. Red
  colours indicate an abundance of software tools in an accuracy and
  speed category, while blue colours indicate scarcity of software in
  an accuracy and speed category. The abundance is quantified using a
  Z-score computation for each bin, this is derived from 10,000 random
  permutations of the speed and accuracy ranks from each
  benchmark. Mean normalised ranks of accuracy and speed have been
  binned into 100 classes (a $10\times10$ grid) that range from
  comparatively slow/inaccurate to comparatively
  fast/accurate. Z-scores with a P-value less than 0.05 are indicated
  with a ‘$\vartimes$’.}
\label{fig:speedaccuracy}
\end{figure*}

There is a major reduction in the number of software tools that are
classed as intermediate in terms of both speed and accuracy based on
permutation tests (see Methods for details, Figure~\ref{fig:speedaccuracy}). The cells
corresponding to the four central deciles are
highlighted {\color{black}(Z = -1.9, -2.5, -2.9 and -1.1, P-values = 0.03, 0.006,
0.002 and 0.14}, respectively, reading from top to bottom, left to
right). We also tested the relative age of the software tools in
the indicated corners and central regions of the speed vs
accuracy plot. We found that the “slow and inaccurate” tools ($N=27$) were
generally published earlier than ``fast and accurate'' tools ($N=23$) {\color{black}(W = 341, P=0.005,
one-tailed Wilcoxon test)} (Figure S7).

\section*{Conclusion}

We have gathered data on the relative speeds and accuracies of
{\color{black}\numTools~} bioinformatic tools from
{\color{black}\numBenchmarkPubs} benchmarks published between
2005 and 2020. Our results provide significant support for the suggestion that there are major benefits to
the long-term support of software development \cite{siepel2019challenges}. The
finding of a strong relationship between the number of commits and code contributors to
GitHub (i.e. software updates) and accuracy, highlights the benefits of
long-term development.

Our study finds little evidence to support that impact-based metrics
have any relationship with software quality, this is unfortunate as these
are frequently cited reasons for selecting software tools
\cite{Loman2015-bw}. This implies that the high citation
rates for bioinformatic software
\cite{Perez-Iratxeta2007-lv,Van_Noorden2014-kc,Wren2016-xy} is more a
reflection of other factors such as user-friendliness or the Matthew Effect
\cite{Lariviere2010-kx,Merton1968-cb} other than accuracy.

We found the lack of a correlation between software speed and accuracy
surprising.  The slower software tools are overrepresented at both
high and low levels of accuracy (Figure~\ref{fig:speedaccuracy}).
In addition, we find an under-representation of software that has
intermediate levels of both accuracy and speed. A possible explanation
for this is that bioinformatic software tools are bound by a form of
publication-bias \cite{Boulesteix2015-am,Nissen:2016}. That is, the
probability that a study being published is influenced by the results
it contains \cite{sterling1995publication}. The community of
developers, reviewers and editors may be unwilling to publish software
that is not highly ranked on speed or accuracy. If correct, this may
have unfortunate consequences.  We also found that slow and inaccurate
software is generally published earlier than fast and accurate tools
{\color{black}(P=0.007, one-tailed Wilcoxon test)}, therefore
comparisons were not required to publish the slow and
inaccurate tools.

%% A
%% gedankenexperiment may be sufficient to show that slow software is
%% less thoroughly tested than fast software. This is because typical
%% academic software development is an iterative process, where tools are
%% refined by successive rounds of coding, testing and evaluation
%% \cite{Wilson2006-ih}. We can assume that similar time-spans are spent
%% on most projects, for example, the span of a PhD degree or
%% Postdoctoral Fellowship.  As a consequence, slow software may undergo
%% fewer development cycles than faster tools.

While we have taken pains to mitigate many issues with our analysis, nevertheless 
some limitations remain. For example, it has proven difficult to verify if the gap in medium accuracy 
and medium speed software is genuinely the result of publication bias, or due to additional factors that we have 
not taken in to account. In addition, all of the features we have used here are moving targets, e.g. as software 
tools are refined, their relative accuracies and speeds will change, the citation metrics, ages, and version control 
derived measures also change over time. We merely report a snapshot of values from 2020.  The benchmarks themselves may 
also introduce biases into the study, for example there are issues with a potential lack of independence between benchmarks 
(e.g. shared datasets, metrics and tools), there are heterogeneous measures of accuracy and speed and often unclear 
processes for including different tools. 

%{\bf DESCRIBE STUDY LIMITATIONS... 
%\begin{itemize}
%    \item Difficult to test publication biases
%    \item Heterogeneous measures of accuracy between benchmarks  
%    \item possible lack of independence between different benchmarks 
%    \item Non-uniform inclusion of tools in different benchmarks -- some tools ranked many times (e.g. velvet and soap assemblers ranked 26 different times)
%    \item How to deal with inconsistently reported versioning of software? 
%    \item Metrics and tools performance are all moving targets, only a snap shot is used here  
%\end{itemize}
%}


We propose that the full spectrum of software tool accuracies and
speeds serve a useful purpose to the research community. Like negative
results, if honestly reported, illustrate to the research community
that certain approaches are not practical research avenues
\cite{fanelli2012negative}. % Ioannidis2005-xh,Workman1999-au,Rivas2000-fb}.
The current
practices of publishers, editors, reviewers and authors of software
tools therefore may be depriving our community of tools for building effective
and productive workflows.

The most reliable way to identify accurate software tools is through neutral
software benchmarks \cite{Boulesteix2013-vb}. We are hopeful that
this, along with steps to reduce the publication-bias we have
described, will reduce the over-optimistic and misleading reporting of
tool accuracy \cite{Boulesteix2010-te,Jelizarow2010-zf,Norel2011-cq}.


\section*{Methods}
In order to evaluate predictors of computational biology software
accuracy, we mined the published literature, extracted data from
articles, connected these with bibliometric databases, and tested for
correlates with accuracy. We outline these steps in further detail
below.

\textbf{Criteria for inclusion:} We are interested in using
computational biology benchmarks that satisfy Anne-Laure Boulesteix’s
(ALB) three criteria for a “neutral comparison study”
\cite{Boulesteix2013-vb}. Firstly, the main focus of the article is
the comparison and \textbf{not} the introduction of a new
tool. Secondly, the authors should be reasonably neutral, which means
that the authors should not generally have been involved in the
development of the tools included in the benchmark. Thirdly, the test
data and evaluation criteria should be sensible. This means that the
test data should be independent of data that tools have been trained
upon, and that the evaluation measures appropriately quantify correct
and incorrect predictions.

Literature mining: We identified an initial list of 10 benchmark
articles that satisfy the ALB-criteria. These were identified based
upon previous knowledge of published articles and were supplemented
with several literature searches (e.g. ‘“benchmark” AND “cputime”’ was
used to query both GoogleScholar and Pubmed
\cite{Sayers2010-vm,McEntyre2001-fl}). We used these articles to seed
a machine-learning approach for identifying further candidate articles
and to identify new search terms to include.

For our machine-learning-based literature screening, we computed a
score ($s(a)$) for each article that tells us the likelihood that it
is a benchmark. In brief, our approaches uses 3 stages:
\begin{enumerate}
\item Remove high frequency words from the title and abstract of candidate articles (e.g. ‘the’, ‘and’, ‘of’, ‘to’, ‘a’, …) 
\item Compute a log-odds score for the remaining words 
\item Use a sum of log-odds scores to give a total score for candidate articles
\end{enumerate}
For stage 1, we identified a list of high frequency (e.g. $f$(word) $>$
$1/10,000$) words by pooling the content of two control texts
\cite{Carroll1865-hk,Tolkien1937-ke}.

For stage 2, in order to compute a log-odds score for bioinformatic
words, we computed the frequency of words that were not removed by our
high frequency filter in two different groups of articles:
bioinformatics-background and bioinformatics-benchmark articles. The
text from bioinformatics-background articles were drawn from the
bioinformatics literature, but these were not necessarily associated
with benchmark studies. For background text we used Pubmed
(\cite{Sayers2010-vm,McEntyre2001-fl} to select 8,908 articles that
contained the word “bioinformatics” in the title or abstract and were
published between 2013 and 2015. We computed frequencies for each word
by combining text from titles and abstracts for the background and
training articles. A log-odds score is computed for each word using
the following formula:
%\todo{Word is denoted $w$ on the left and $word$ on the right.}
$lo(word)=\log_2\frac{f_{tr}(word)+\delta}{f_{bg}(word)+\delta}$, where
$\delta$
is a pseudo-count added for each word ($\delta = 10^{-5}$, by default),
$f_{bg}(word)$ and $f_{tr}(word)$ are the frequencies of a $word$ in
the background and training datasets respectively. Word frequencies
are computed by counting the number of times a word appears in the
pool of titles and abstracts, the counts are normalised by the total
number of words in each set.

Thirdly, we also collected a group of candidate benchmark articles by
mining Pubmed for articles that are likely to be benchmarks of
bioinformatic software, these may match the terms: “((bioinformatics)
AND (algorithms OR programs OR software)) AND (accuracy OR assessment
OR benchmark OR comparison OR performance) AND (speed OR
time)”. Further terms used in this search were progressively added as
relevant enriched terms were identified in later iterations. The final
query is given in \textbf{supplementary materials}.

A score is computed for each candidate article by summing the log-odds
scores for the words in title and abstract,
i.e. $s(a)=\sum_i^Nlo(w_i)$. The high scoring candidate articles are
then manually evaluated against the ALB-criteria. Accuracy and speed
ranks are extracted from the articles that meet the criteria, and
these are also added to the set of training articles. The evaluated
candidate articles that do not meet the ALB-criteria are incorporated
into the set of background articles. This process is iterated and has resulted in the identification of
{\color{black}\numBenchmarkPubs} benchmark articles, that
contain {\color{black}\numBenchmarks} different benchmarks, together these
rank {\color{black}\numTools~} distinct software packages.

There is a potential for bias to have been introduced into this
dataset. Some possible forms of bias include converging on a niche
group of benchmark studies due to the literature mining technique that
we have used. A further possibility is that benchmark studies
themselves are biased, either including very high performing or very
low performing software tools. To address each of these concerns we
have attempted to be as comprehensive as possible in terms of
benchmark inclusion, as well as include comprehensive benchmarks. By
which we mean studies that include all available software tools that
address a biological problem.

\textbf{Data extraction and processing:} for each article that met the
ALB-criteria and contained data on both the accuracy and speed from
their tests we extracted ranks for each tool. Many articles contained
multiple benchmarks, in these cases we selected a range of these, the
provenance of which is stored with the accuracy metric and raw speed
and accuracy rank data for each tool. In line with rank-based
statistics, the cases where tools were tied are resolved by using a
midpoint rank (e.g. if tool 3 and 4 are tied, the rank 3.5 is used)
\cite{Mann1947-re}. Each rank extraction was independently verified by
at least one other co-author to ensure both the provenance of the data
could be established and that the ranks were correct. The ranks for
each benchmark were then normalised to lie between 0 and 1 using the
formula $1-\frac{r-1}{n-1}$ where ‘$r$’ is a tool’s rank and ‘$n$’ is the
number of tools in the benchmark. For tools that were benchmarked
multiple times with multiple metrics (e.g. BWA is evaluated in 6
different articles
\cite{Bao2011-lv,Caboche2014-lj,Hatem2013-cs,Schbath2012-ob,Ruffalo2011-rl,Holtgrewe2011-fd})
a mean normalised rank is used to summarise the accuracy and speed performance. 
Or, more formally:
 
\begin{equation*}
\begin{split}
accuracy =& \sum_{i=1..N} 1-\frac{r^{accuracy}_i-1}{n_i-1}, \\
speed    =& \sum_{i=1..N} 1-\frac{r^{speed   }_i-1}{n_i-1}
\end{split}
\end{equation*}
 
For each tool we identified the corresponding publications in
GoogleScholar, the total number of citations was recorded, the
corresponding authors were also identified and if these had public
GoogleScholar profiles we extracted their H-index and calculated a
M-index ($\frac{H-index}{y}$) where ‘$y$’ is the number of years since
their first publication. For the journals that each tool is published
in we extracted the H5-index
from GoogleScholar Metrics. The year of publication was also recorded for each
tool. A “relative age” and “relative citations” was also computed for
each tool. For each benchmark, software was ranked by year of first
publication (or number of citations), ranks were assigned and then
normalised as described above. Tools ranked in multiple evaluations
were then assigned a mean value for “relative age” and “relative
citations”.

The papers describing tools were checked for information on version numbers and links to GitHub. 
Google was also employed to identify GitHub repositories. When a repository was matched with a tool, the number of "commits" and number of "contributors" was collected, when details of version numbers were provided, these were also harvested. 
Version numbers are inconsistently used between groups, and may begin at either 0 or 1. To counter this issue we have added '1' to all versions less than '1', e.g. version 0.31 become 1.31. In addition, multiple point releases may be used e.g. 'version 5.2.6', these have been mapped to the nearest decimal value '5.26'.

\textbf{Statistical analysis:} For each tool we have manually collected up to 12 different
statistics from GoogleScholar, GitHub and directly from literature describing tools (
1. corresponding author’s H-index, 
2. corresponding author’s M-index, 
3. journal H5 index, 
4. normalised accuracy rank, 
5. normalised speed rank, 
6. number of citations, 
7. relative age, 
8. relative number of citations, 
9. year first published, 
10. version
11. number of commits to GitHub,
12. number of contributors to GitHub). 
These have been evaluated in a pairwise fashion to
produce Figure~\ref{fig:allfactors} A\&B, the R code for these is
given in the GitHub repository (linked below).

%The linear models that we used to test for relationships between
%speed, accuracy and the above measures are:
%
%\begin{equation*}
%\begin{split}
%accuracy=& c_0+c_1\times speed+c_2\times JIF+c_3\times H5+\\
%& c_4\times citations+c_5\times Hindex+\\
%& c_6\times Mindex+c_7\times relativeAge+\\
%& c_8\times relativeCitations
%\end{split}
%\end{equation*}
%
%\begin{equation*}
%\begin{split}
%speed=& c_0+c_{1}\times accuracy+c_{2}\times JIF+c_{3}\times H5+\\
%& c_{4}\times citations+c_{5}\times Hindex+\\
%& c_{6}\times Mindex+c_{7}\times relativeAge+\\
%& c_{8}\times relativeCitations
%\end{split}
%\end{equation*}


For each benchmark of three or more tools, we extracted the published
accuracy and speed ranks. In order to identify if there is an
enrichment of certain accuracy and speed pairings we constructed a
permutation test. The individual accuracy and speed ranks were
reassigned to tools in a random fashion and each new accuracy and
speed rank pairing was recorded. For each benchmark this procedure was
repeated 10,000 times. These permuted rankings were normalised and
compared to the real rankings to produce the ‘$\vartimes$’ points in
Figure~\ref{fig:allfactors}B and the heatmap and histograms in
Figure~\ref{fig:speedaccuracy}. The heatmap in
Figure~\ref{fig:speedaccuracy} is based upon Z-scores
($Z=\frac{x-\bar{x}}{\sigma}$). For each cell in a $10\times 10$ grid
a Z-score is computed to illustrate the abundance or lack of tools in
a cell relative to the permuted data.

\section*{Data availability}
Raw datasets, software and documents are available under a CC-BY license:\\
%\fussy
%\url{https://docs.google.com/spreadsheets/d/14xIY2PHNvxmV9MQLpbzSfFkuy1RlzDHbBOCZLJKcGu8/edit?usp=sharing}\\
%and here:\\
%\fussy
%\url{https://dx.doi.org/10.6084/m9.figshare.4299320.v1}\\
%\sloppy
%Additional documentation, code, figures and raw data is available here:\\
\fussy
\url{https://github.com/Gardner-BinfLab/speed-vs-accuracy-meta-analysis}
\sloppy

\section*{Acknowledgements}

The authors acknowledge the valued contribution of invaluable
discussions with Anne-Laure Boulesteix, Shinichi Nakagawa, Suetonia
Palmer and Jason Tylianakis. Murray Cox, Raquel
Norel, Alexandros Stamatakis, Jens Stoye,Tandy Warnow, provided
valuable feedback on drafts of the manuscript.

PPG is supported by a Rutherford Discovery Fellowship,
administered by the Royal Society Te Ap\=arangi, 
PPG, AG and MB acknowledge support from a Data Science Programmes grant
(UOAX1932).

%{\bf AND ETC....}

%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\bibliographystyle{unsrt}
\bibliography{references.bib}





\end{document}
