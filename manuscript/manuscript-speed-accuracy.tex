\documentclass[fleqn,10pt]{SelfArx} % Document font size and equations flushed left

\usepackage{float}
\usepackage{graphicx}
\usepackage{caption}
\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{amsmath}
\graphicspath{{../figures/}}
\setlength{\abovecaptionskip}{15pt plus 3pt minus 2pt}

\newcommand{\beginsupplement}{%
        \setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}%
        \setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}%
     }

\definecolor{color1}{RGB}{0,0,90} % Color of the article title and sections
\definecolor{color2}{RGB}{200,200,200} % Color of the boxes behind the abstract and headings
\definecolor{color3}{RGB}{5,5,5} % Color of the boxes behind the abstract and headings

\JournalInfo{.} % Journal information ``Journal, Vol. XXI, No. 1, 1-5, 2015''
\Archive{ } % Additional notes (e.g. copyright, DOI, review/research article)

\def\numTools{499}
%cat meanRankSpeedData.tsv | cut -f 4 | grep -v ^method$ | wc -l
\def\numBenchmarkPubs{69}
%cat speed-vs-accuracy-toolRanks2005-2020.tsv | cut -f 1 | sort  | uniq | grep -E '[0-9]'  | wc -l
\def\numBenchmarks{134}
%cat rawRankSpeedData2005-2020.tsv | cut -f 1 | grep -v ^testId$ | uniq -c | wc -l

\PaperTitle{A meta-analysis of bioinformatics software benchmarks reveals that publication-bias unduly influences software accuracy} % Article title

\Authors{Paul P. Gardner\textsuperscript{1,2}*, James M. Paterson\textsuperscript{?}, Fatemeh Ashari-Ghomi\textsuperscript{?}, Sinan U. Umu\textsuperscript{?}, Stephanie McGimpsey\textsuperscript{3,4}, Aleksandra Pawlik\textsuperscript{5},
Alex Gavryushkin\textsuperscript{6}, Michael A Black\textsuperscript{1}
} % Authors

\affiliation{\textsuperscript{1}\textit{Department of Biochemistry, University of Otago, Dunedin, New Zealand.}} % Author affiliation
\affiliation{\textsuperscript{2}\textit{Biomolecular Interaction Centre, University of Canterbury, Christchurch, New Zealand.}}
\affiliation{\textsuperscript{3}\textit{McConomy School of Dance, Derry, Ireland.}}
\affiliation{\textsuperscript{4}\textit{Research for Good, Limavady, Ireland.}}
\affiliation{\textsuperscript{5}\textit{New Zealand eScience Infrastructure, 49 Symonds St, Auckland, New Zealand.}}
\affiliation{\textsuperscript{6}\textit{Department of Computer Science, University of Otago, Dunedin, New Zealand.}} % Author affiliation
\affiliation{*\textbf{Corresponding author}: paul.gardner@otago.ac.nz} % Corresponding author

\Keywords{} % Keywords - if you don't want any simply remove all the text between the curly brackets
\newcommand{\keywordname}{Keywords} % Defines the keywords heading name

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

%\Abstract{In the below we provide additional results for our investigation of computational biology benchmarks.}
\Abstract{ Computational biology has provided widely used and powerful
  software tools for testing and making inferences about biological
  data. In the face of rapidly increasing volumes of data, heuristic
  methods that trade software speed for mathematical completeness must
  be employed.  We are interested in testing for trade-offs between
  speed and accuracy and for other factors are indicative of accurate
  software.

We have extracted accuracy and speed ranks from independent benchmarks of
computational biology software of different software tools, and evaluate the
factors that are likely to influence accuracy e.g. speed, author
reputation, journal impact, recency or developer efforts.

We found that software speed, author reputation, journal impact, the
number of citations and age are all unreliable predictors of software
accuracy. This is unfortunate because citations, author and journal
reputation are frequently used reasons for selecting software
tools. Our results do show that accurate bioinformatic software tools
are generally the product many gradual improvements, often from
multiple developers, based upon github records. In addition, we find
that bioinformatics has an excess of slow and inaccurate software
tools across many sub-disciplines. Meanwhile, there are few tools that
are middle-of-road in terms of accuracy and speed trade-offs. We
hypothesise that a form of publication-bias unduly influences the
publication and development of bioinformatic software tools. In other
words, at present software that is not highly ranked on speed and not
highly ranked on accuracy is difficult to publish due to author,
editor and reviewer practices. This leaves an unfortunate gap in the
literature upon which future software refinements cannot be
constructed.  }

\begin{document}

\flushbottom % Makes all text pages the same height
\maketitle % Print the title and abstract box
%\tableofcontents % Print the contents section

\thispagestyle{empty} % Removes page numbering from the first page

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

%\onecolumn
%\beginsupplement



%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------



%%REFERENCES::
%reference Stefan Buchka$^{1}$, Rory Wilson$^2$, Alexander Hapfelmeier$^3$,\\  Paul Gardner$^4$,  Anne-Laure Boulesteix PAPER!!!

%% A comprehensive analysis of the usability and archival stability of omics computational tools and resources
%% https://www.biorxiv.org/content/10.1101/452532v1

%% Tools and best practices for data processing in allelic expression analysis
%% https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0762-6

%% Towards reproducible, transparent, and systematic benchmarking of omics computational tools
%% https://osf.io/p8yd9

%% Essential guidelines for computational method benchmarking
%% https://arxiv.org/abs/1812.00661

\section*{Background}
Computational biology software is widely used and has produced some of
the most cited publications in the entire scientific corpus
\cite{Perez-Iratxeta2007-lv,Van_Noorden2014-kc,Wren2016-xy}. This
highly-cited software includes implementations of methods for sequence
alignment and homology inference
\cite{Altschul1990-ht,Thompson1994-eu,Thompson1997-rl,Altschul1997-ga},
phylogenetic analysis
\cite{Felsenstein1985-lj,Saitou1987-zl,Posada1998-qq,Ronquist2003-yh,Tamura2007-ei},
statistical analysis of survival patterns in biomedicine
\cite{Kaplan1958-ju,Cox1972-nu}, biomolecular structure analysis
\cite{Sheldrick1990-kc,Sheldrick2008-xy,Jones1991-ik,Laskowski1993-vi,Otwinowski1997-xj},
visualization and data collection
\cite{Kraulis1991-lt,Berman2000-to}. However, the popularity of a
software tool does not necessarily mean that it is accurate or
computationally efficient, instead usability, ease of installation,
operating system and other factors may play a greater role in a software
tool's popularity. There have been several notable incidences where
inaccurate software has caused considerable harm
\cite{leveson1993investigation}.
%Fergusson covid predictions? 

Progress in the biological sciences is increasingly limited by the
ability to analyse increasing volumes of data, therefore the
dependence of biologists on software is also increasing
\cite{Marx2013-zi}. There is an increasing use of technological
solutions for automating biological data generation
(e.g. next-generation sequencing, mass-spectroscopy, cell-tracking and
species tracking), therefore the biological sciences have become
increasingly dependent upon computational software for processing
large quantities of data \cite{Marx2013-zi}. As a consequence, the
computational efficiency of data processing and analysis software is
of great importance to decrease the energy and time costs of research
\cite{Gombiner2011-md}. Furthermore, even small error rates can have a
major impact on the number of false inferences as datasets become
larger \cite{Storey2003-cv}.

The gold-standard for determining accuracy is for independent
researchers to conduct benchmarks, which can serve a useful role in
reducing the over-optimistic reporting of software accuracy
\cite{Boulesteix2010-te,Jelizarow2010-zf,Weber:2019} and the self-assessment trap
\cite{Norel2011-cq,Buchka:2021}. Benchmark studies typically use a number of
positive and negative control datasets, predictions from different software tools can then be
partitioned into true or false groups and a variety of metrics can be
used to evaluate the performance differences
\cite{Egan1975-nd,Hall2012-kg,Weber:2019}.
%Some benchmarks now use live, or
%frequently updated, results to indicate the latest developments in
%software performance
%\cite{Bujnicki2001-xr,Puton2014-hy,Barton_undated-er}.
The aim of these benchmarks is to independently identify tools that
make acceptable compromises in terms of balancing speed with
discriminating true and false predictions, and are therefore suited
for wide adoption by the community.

For common computational biology tasks, a proliferation of
software-based solutions often exists
\cite{Felsenstein1995-ic,Altschul2013-bv,Henry2014-ut,Wikipedia_contributors2015-vj,Wikipedia_contributors2015-hr}. While
this is a good problem to have, and points to a
diversity of options from which practical solutions can be selected,
many possible options creates a dilemma for users. In the absence of
any recent gold-standard benchmarks, how should scientific software be
selected? In the following we presume that ``biological
accuracy'' is the most desirable feature for a software tool. Biological
accuracy is the degree to which predictions or measurements reflect
the biological truths based on expert-derived curated datasets. In some
fields biological accuracy is very difficult to ascertain, for
example, in phylogenetics it is nearly impossible to know the
true ancestral relationships between organisms. In situations like this,
researchers may use simulated or high-confidence datasets.

A number of possible predictors of software quality are used by the
community of computational biology software users. Some accessible,
quantifiable and frequently used proxies for identifying high quality
software include: \textbf{1. Recency:} recently published software
tools may have built upon the results of past work or be an update to
an existing software. Therefore, these could be more accurate and
faster. \textbf{2. Wide adoption:} a software tool may be widely used
because it is fast and accurate, or because it is well-supported and
user-friendly. In fact, “large user base”, “word-of-mouth”,
“wide-adoption”, "personal recommendation," and "recommendation from a
close colleague," are frequent responses to surveys of “how do
scientists select software?”
\cite{Hannay2009-cf,Joppa2013-vj,Loman2015-bw}. \textbf{3. Journal
  impact:} high profile journals are run by editors and reviewers who
carefully select and curate the best manuscripts. Therefore, high
impact journals may be more likely to select manuscripts describing
good software \cite{Garfield1955-wf}. \textbf{4. Author/Group
  reputation:} the key to any project is the skills of the people
involved, including maintaining a high collective intelligence
\cite{Joppa2013-vj,Woolley2010-ld,Cheruvelil2014-xn}. As a
consequence, an argument could be made that well respected and
high-profile authors will produce better software
\cite{Hirsch2005-mt,Bornmann2008-il}. \textbf{5. Speed:} software tools
frequently trade accuracy for speed. For example, heuristic
software such as the popular homology search tool, BLAST, compromise
the mathematical guarantee of optimal solutions for more speed
\cite{Altschul1990-ht,Altschul1997-ga}. Some researchers may naively
interpret this fact as slower software is likely to be more
accurate. But speed may also be influenced by the programming language
\cite{Fourment2008-vl}, and the level of hardware optimisation
\cite{Farrar2007-ky,Dematte2010-ph}; however, the implementation is
likely to have more of an impact (e.g. brute-force approaches versus
rapid and sensitive pre-filtering
\cite{Schaeffer1989-mu,Papadimitriou_undated-bo}).

Other factors that are less quantifiable that influence whether a
software tool is selected include: whether the documentation is good,
user-friendly, word-of-mouth and ``used in a similar analysis''
\cite{Loman2015-bw}.
%This information is not as readily
%quantifiable as the above measures. However,
However, citation metrics may be a
useful proxy for the above.
%This may also explain the reason why
%some software continues to be used, in spite of poor relative
%performance \cite{Wadi2016-dj}.
%We note that none of these factors
%are directly linked to the top ranked features of successful software
%projects, as based upon literature surveys \cite{nasir2011critical}.

With the wide adoption of Github ($\approx 47\%$ of the
\textbf{{\color{red}\numTools}} tools included in this study could be
linked to a Github repository), quantifiable data on software
development such as the number of contributors to code, number of code
changes and versions is also available
\cite{ray2014large,Dozmorov:2018,mangul2018comprehensive}.

In the following study, we explore factors that may be indicative of
software accuracy. This, in our opinion, should be one of the prime
reasons for selecting a software tool. We have mined the large and
freely accessible PubMed database \cite{Sayers2010-vm} for benchmarks
of computational biology software, and manually extracted accuracy and
speed rankings for \textbf{{\color{red}\numTools~}} unique software tools. For
each tool, we have collected measures that may be predictive
of accuracy, and may be subjectively employed by the researcher
community as a proxy for software quality. These include relative
speed, relative age, the productivity and impact of the corresponding
authors, journal impact and the number of citations.

\section*{Results}
We have collected relative accuracy and speed ranks for
\textbf{{\color{red}\numTools~}} distinct software tools. This software has
been developed for solving a broad cross-section computational biology
tasks.
%% These include software for homology search
%% \cite{Freyhult2007-et}, genome sequence analysis (e.g. read mapping or
%% sequence assembly)
%% \cite{Junemann2014-mb,Tran2014-pe,Zhang2011-nd,Abbas2014-gu,Bao2011-lv,Caboche2014-lj,Kleftogiannis2013-wi,Hatem2013-cs,Schbath2012-ob,Ruffalo2011-rl,Yang2013-aj,Holtgrewe2011-fd,Rackham2015-ag,Huang2015-wu},
%% multiple sequence alignment
%% \cite{Thompson2011-rf,Nuin2006-nk,Pais2014-sr,Pervez2014-zp,Liu2010-rp},
%% cell tracking \cite{Maska2014-ak}, transcriptome analysis
%% \cite{Li2012-wr,Lu2013-fs,Liu2014-kz,Kumar2016-xz}, RNA interaction
%% prediction \cite{Pain2015-gr}, protein interactions
%% \cite{Tikk2010-qd}, protein structure prediction
%% \cite{Kolodny2005-ry,Wallner2005-qi}, epistasis \cite{Shang2011-vy},
%% metagenomic analysis \cite{Lindgreen2016-tt,Bazinet2012-wf},
%% repetitive sequence prediction \cite{Saha2008-kd}, proteomics
%% \cite{Lange2008-pt,Yang2009-oc} and phylogenetics
%% \cite{Liu2011-pz,Yang2011-dv,Oscamou2008-md,Bayzid2013-hc,Liu2009-lx}.
Each
software tool was benchmarked in at least one of
\textbf{{\color{red}\numBenchmarkPubs~}} publications that satisfy the Boulesteix
criteria \cite{Boulesteix2013-vb}. In brief, the Boulesteix criteria
are: 1. the main focus of the article is a benchmark. 2. the authors
are reasonably neutral. 3. the test data and evaluation criteria are
sensible.

For each of the publications describing these tools, we have (where
possible) the H5-index published by
\href{https://scholar.google.co.nz/citations?view\_op=top\_venues\&hl=en}{Google
  Scholar Metrics}. We have collected the maximum H-index and
corresponding M-indices \cite{Hirsch2005-mt} for the corresponding
authors for each tool, and the number of times the publication(s)
associated with a tool has been cited using Google Scholar (data
collected over a 6 month period in late 2020). Note that the citation metrics
are not static and will change over time. 

We have computed the Spearman’s correlation coefficient for each
pairwise combination of the mean normalised accuracy and speed ranks,
the year published, mean relative age (compared to software in the
same benchmarks), journal H5 metrics, the total number of citations,
the relative number of citations (compared to software in the same
benchmarks) and the maximum H- and corresponding M-indices for the corresponding
authors. Where possible we also extract the version number, the number
of commits and number of contributors from Github repositories.  The
results are presented in Figure~\ref{fig:allfactors}A. We find
significant associations between most of the citation-based metrics
(journal H5, citations, relative citations, H-index and
M-index). There is also a negative correlation between the year of
publication, the relative age and many of the citation-based metrics.

\begin{figure*}[h]
\includegraphics[width=\textwidth]{figure1.pdf}
\caption{\textbf{A.} A heatmap indicating the relationships between
  proposed predictors of software quality. Spearman’s rho is used to
  infer correlations between metrics such as the H- and M-indices of
  corresponding authors, number of citations, journal impact factors
  and H5 indices, the year and relative age of software and the mean
  relative rankings of software speed and accuracy. Red colours
  indicate a positive correlation, blue colours indicate a negative
  correlation. Correlations with a P-value less than 0.05 are
  indicated with a ‘X’. The dashed rectangular area is illustrated in
  more detail in \textbf{B}, the relationship between speed and
  accuracy is shown in more detail in \textbf{Figure 2}.
  %The
  %dendrogram was computed using the default ‘heatmap.2 function in R,
  %which computes a Euclidean distance matrix and a complete-linkage
  %hierarchical clustering.
  \textbf{B.} A barplot illustrating the correlation, as measured by
  Spearman’s rho, between normalised accuracy ranks and software tool
  features that may be predictive of accuracy. In order to give an
  appreciation of the difference between the observed effect-sizes and
  significant effect sizes, we generated 1000 permuted accuracy ranks
  for each benchmark and recorded Spearman’s rho for the significant
  correlations (P<0.05). These values are marked with “x”s in the
  barplot. Significant correlations are marked with red asterisks.}
\label{fig:allfactors}
\end{figure*}

Data on the
number of updates to software tools from Github such as the number of versions,
commits and contributors \textbf{{\color{red}was significantly correlated with 
accuracy (respective Spearman’s rhos = 0.13, 0.22, 0.21, respective P-values = 0.0027, 0.00079, 0.0013). 
These features were not correlated with speed however (see Figure~\ref{fig:allfactors}A).
}}
We also found that author reputation metrics, journal impacts and the age
of tools were generally \textbf{not} correlated with either tool
accuracy or speed (see Figure~\ref{fig:allfactors}). The strongest
association was between accuracy and \textbf{{\color{red}the relative
    number of citations}} (Spearman’s rho = \textbf{{\color{red}0.092,
    P-value = 0.047}}). But the effect size is only just over the
significance threshold and may be explained by multiple-testing. 
    
%To further validate this result, we compute a correlation
%between speed and accuracy for each benchmark and used weighted sum
%Z-tests \cite{Zaykin2011-tj}. This also failed to identify a
%significant relationship (sum Z=-0.1, P-value=0.5).

In order to gain a deeper understanding of the distribution of
available bioinformatic software tools on a speed versus accuracy
landscape, we ran a permutation test. The ranks extracted
from each benchmark were randomly permuted, generating 1,000
randomized speed and accuracy ranks. In the cells of a $10\times10$
grid spanning the normalised speed and accuracy ranks we computed a
Z-score for the observed number of tools in a cell, compared to the
expected distributions generated by 1,000 randomized ranks. The results
of which are shown in Figure~\ref{fig:speedaccuracy}. We identified \textbf{{\color{red}18}}
bins where there was a significant excess or dearth of tools. For
example, there was an excess of “slow and inaccurate” software (\textbf{{\color{red}Z=2.50,
P-value=0.0063}}). We find that the amount of software classed as “fast
and accurate” and “fast and inaccurate” are at approximately the
expected proportions based upon the permutation test. The number of
significant results is in excess, and is not due to multiple testing, as
the probability of finding \textbf{{\color{red}18}} of 100 tests significant by chance is
low (P-value = \textbf{{\color{red}$2.2\times 10^{-6}$}}, exact binomial test).

% binom.test(sigCount,gridRes^2, p = 0.05)

\begin{figure*}
%\includegraphics[scale=0.55]{figure2.pdf}
\includegraphics[width=\textwidth]{figure2.pdf}
\caption{A heatmap indicating the relative paucity or abundance of
  software in the range of possible accuracy and speed rankings. Red
  colours indicate an abundance of software tools in an accuracy and
  speed category, while blue colours indicate scarcity of software in
  an accuracy and speed category. The abundance is quantified using a
  Z-score computation for each bin, this is derived from 10,000 random
  permutations of the speed and accuracy ranks from each
  benchmark. Mean normalised ranks of accuracy and speed have been
  binned into 100 classes (a $10\times10$ grid) that range from
  comparatively slow/inaccurate to comparatively
  fast/accurate. Z-scores with a P-value less than 0.05 are indicated
  with a ‘X’.}
\label{fig:speedaccuracy}
\end{figure*}

There is a major reduction in the number of software tools that are
classed as intermediate in terms of both speed and accuracy based on
permutation tests (see Methods for details, Figure~\ref{fig:speedaccuracy}). The cells
corresponding to the four central deciles are
highlighted \textbf{{\color{red}(Z = -1.9, -2.5, -2.9 and -1.1, P-values = 0.03, 0.006,
0.002 and 0.14}}, respectively, reading from top to bottom, left to
right). We also tested the relative age of the software tools in
the indicated corners and central regions of the speed vs
accuracy plot. We found that the “slow and inaccurate” tools ($N=27$) were
generally published earlier than ``fast and accurate'' tools ($N=23$) \textbf{{\color{red}(W = 341, P=0.005,
one-tailed Wilcoxon test)}} (Figure S7).

\section*{Conclusion}

We have gathered data on the relative speeds and accuracies of
\textbf{{\color{red}\numTools~}} bioinformatic tools from
\textbf{{\color{red}\numBenchmarkPubs}} benchmarks published between
2005 and 2020. {\bf Our results suggest there are major benefits for
  long term support for software \cite{siepel2019challenges}.} The
finding of a strong relationship between the number of commits to
github (i.e. software updates) and accuracy, highlights the benefits of
long-term development.

Our study finds little evidence to support that impact-based metrics
are poorly related to software quality, this is unfortunate as these
are frequently cited reasons for selecting software tools
\cite{Loman2015-bw}. This implies that the recorded high citation
rates for bioinformatic software
\cite{Perez-Iratxeta2007-lv,Van_Noorden2014-kc,Wren2016-xy} is more a
reflection of user-friendliness and possibly the Matthew Effect
\cite{Lariviere2010-kx,Merton1968-cb} than accuracy.

We found the lack of a correlation between software speed and accuracy
surprising.  The slower software tools are overrepresented at both
high and low levels of accuracy (Figure~\ref{fig:speedaccuracy}).
In addition, we find an under-representation of software that has
intermediate levels of both accuracy and speed. A possible explanation
for this is that bioinformatic software tools are bound by a form of
publication-bias \cite{Boulesteix2015-am,Nissen:2016}. That is, the
probability that a study being published is influenced by the results
it contains \cite{sterling1959publication}. The community of
developers, reviewers and editors may be unwilling to publish software
that is not highly ranked on speed or accuracy. If correct, this may
have unfortunate consequences.  We also found that slow and inaccurate
software is generally published earlier than fast and accurate tools
\textbf{{\color{red}(P=0.007, one-tailed Wilcoxon test)}}, therefore
comparisons were not required to publish the slow and
inaccurate tools.

%% A
%% gedankenexperiment may be sufficient to show that slow software is
%% less thoroughly tested than fast software. This is because typical
%% academic software development is an iterative process, where tools are
%% refined by successive rounds of coding, testing and evaluation
%% \cite{Wilson2006-ih}. We can assume that similar time-spans are spent
%% on most projects, for example, the span of a PhD degree or
%% Postdoctoral Fellowship.  As a consequence, slow software may undergo
%% fewer development cycles than faster tools.

{\bf STUDY LIMITATIONS...}


We propose that the full spectrum of software tool accuracies and
speeds serve a useful purpose to the research community. Like negative
results, if honestly reported, illustrate to the research community
that certain approaches are not practical research avenues
\cite{Ioannidis2005-xh,Workman1999-au,Rivas2000-fb}.
The current
practises of publishers, editors, reviewers and authors of software
tools therefore deprive our community of tools for building effective
and productive workflows.

The most reliable way to select software tools is through neutral
software benchmarks \cite{Boulesteix2013-vb}. We are hopeful that
this, along with steps to reduce the publication-bias we have
described, will reduce the over-optimistic and misleading reporting of
tool accuracy \cite{Boulesteix2010-te,Jelizarow2010-zf,Norel2011-cq}.


\section*{Methods}
In order to evaluate predictors of computational biology software
accuracy, we mined the published literature, extracted data from
articles, connected these with bibliometric databases, and tested for
correlates with accuracy. We outline these steps in further detail
below.

\textbf{Criteria for inclusion:} We are interested in using
computational biology benchmarks that satisfy Anne-Laure Boulesteix’s
(ALB) three criteria for a “neutral comparison study”
\cite{Boulesteix2013-vb}. Firstly, the main focus of the article is
the comparison and \textbf{not} the introduction of a new
tool. Secondly, the authors should be reasonably neutral, which means
that the authors should not generally have been involved in the
development of the tools included in the benchmark. Thirdly, the test
data and evaluation criteria should be sensible. This means that the
test data should be independent of data that tools have been trained
upon, and that the evaluation measures appropriately quantify correct
and incorrect predictions.

Literature mining: We identified an initial list of 10 benchmark
articles that satisfy the ALB-criteria. These were identified based
upon previous knowledge of published articles and were supplemented
with several literature searches (e.g. ‘“benchmark” AND “cputime”’ was
used to query both GoogleScholar and Pubmed
\cite{Sayers2010-vm,McEntyre2001-fl}). We used these articles to seed
a machine-learning approach for identifying further candidate articles
and to identify new search terms to include.

For our machine-learning-based literature screening, we computed a
score ($s(a)$) for each article that tells us the likelihood that it
is a benchmark. In brief, our approaches uses 3 stages:
\begin{enumerate}
\item Remove high frequency words from the title and abstract of candidate articles (e.g. ‘the’, ‘and’, ‘of’, ‘to’, ‘a’, …) 
\item Compute a log-odds score for the remaining words 
\item Use a sum of log-odds scores to give a total score for candidate articles
\end{enumerate}
For stage 1, we identified a list of high frequency (e.g. f(word) $>$
1/10,000) words by pooling the content of two control texts
\cite{Carroll1865-hk,Tolkien1937-ke}.

For stage 2, in order to compute a log-odds score for bioinformatic
words, we computed the frequency of words that were not removed by our
high frequency filter in two different groups of articles:
bioinformatics-background and bioinformatics-benchmark articles. The
text from bioinformatics-background articles were drawn from the
bioinformatics literature, but these were not necessarily associated
with benchmark studies. For background text we used Pubmed
(\cite{Sayers2010-vm,McEntyre2001-fl} to select 8,908 articles that
contained the word “bioinformatics” in the title or abstract and were
published between 2013 and 2015. We computed frequencies for each word
by combining text from titles and abstracts for the background and
training articles. A log-odds score is computed for each word using
the following formula:
$lo(w)=\log_2\frac{•f_{tr}(word)+\delta}{f_{bg}(word)+\delta}$, where
is a prior probability ($\delta = 10^{-5}$, by default),
$f_{bg}(word)$ and $f_{tr}(word)$ are the frequencies of a $word$ in
the background and training datasets respectively. Word frequencies
are computed by counting the number of times a word appears in the
pool of titles and abstracts, the counts are normalised by the total
number of words in each set.

Thirdly, we also collected a group of candidate benchmark articles by
mining Pubmed for articles that are likely to be benchmarks of
bioinformatic software, these may match the terms: “((bioinformatics)
AND (algorithms OR programs OR software)) AND (accuracy OR assessment
OR benchmark OR comparison OR performance) AND (speed OR
time)”. Further terms used in this search were progressively added as
relevant enriched terms were identified in later iterations. The final
query is given in \textbf{supplementary materials}.

A score is computed for each candidate article by summing the log-odds
scores for the words in title and abstract,
i.e. $s(a)=\sum_i^Nlo(w_i)$. The high scoring candidate articles are
then manually evaluated against the ALB-criteria. Accuracy and speed
ranks are extracted from the articles that meet the criteria, and
these are also added to the set of training articles. The evaluated
candidate articles that do not meet the ALB-criteria are incorporated
into the set of background articles. This process is iterated a number
of times and has resulted in the identification of
\textbf{{\color{red}\numBenchmarkPubs}} benchmark articles, that
contain \textbf{{\color{red}\numBenchmarks}} different benchmarks, together these
rank \textbf{{\color{red}\numTools~}} distinct software packages.

There is a potential for bias to have been introduced into this
dataset. Some possible forms of bias include converging on a niche
group of benchmark studies due to the literature mining technique that
we have used. A further possibility is that benchmark studies
themselves are biased, either including very high performing or very
low performing software tools. To address each of these concerns we
have attempted to be as comprehensive as possible in terms of
benchmark inclusion, as well as include comprehensive benchmarks. By
which we mean studies that include all available software tools that
address a biological problem.

\textbf{Data extraction and processing:} for each article that met the
ALB-criteria and contained data on both the accuracy and speed from
their tests we extracted ranks for each tool. Many articles contained
multiple benchmarks, in these cases we selected a range of these, the
provenance of which is stored with the accuracy metric and raw speed
and accuracy rank data for each tool. In line with rank-based
statistics, the cases where tools were tied are resolved by using a
midpoint rank (e.g. if tool 3 and 4 are tied, the rank 3.5 is used)
\cite{Mann1947-re}. Each rank extraction was independently verified by
at least one other co-author to ensure both the provenance of the data
could be established and that the ranks were correct. The ranks for
each benchmark were then normalised to lie between 0 and 1 using the
formula $1-\frac{r-1}{n-1}$ where ‘r’ is a tool’s rank and ‘n’ is the
number of tools in the benchmark. For tools that were benchmarked
multiple times with multiple metrics (e.g. BWA is evaluated in 6
different articles
\cite{Bao2011-lv,Caboche2014-lj,Hatem2013-cs,Schbath2012-ob,Ruffalo2011-rl,Holtgrewe2011-fd})
a mean normalised rank is used to summarise the performance.
 
For each tool we identified the corresponding publications in
GoogleScholar, the total number of citations was recorded, the
corresponding authors were also identified and if these had public
GoogleScholar profiles we extracted their H-index and calculated a
M-index ($\frac{H-index}{y}$) where ‘$y$’ is the number of years since
their first publication. For the journals that each tool is published
in we extracted the “journal impact factor” (JIF) and the H5-index
from Thompson-Reuters and GoogleScholar Metrics databases
respectively. The year of publication was also recorded for each
tool. A “relative age” and “relative citations” was also computed for
each tool. For each benchmark, software was ranked by year of first
publication (or number of citations), ranks were assigned and then
normalised as described above. Tools ranked in multiple evaluations
were then assigned a mean value for “relative age” and “relative
citations”.

\textbf{Statistical analysis:} For each tool we have up to 10
statistics (1. corresponding author’s H-index, 2. corresponding
author’s M-index, 3. journal H5 index, 4. journal impact factor,
5. normalised accuracy rank, 6. normalised speed rank, 7. number of
citations, 8. relative age, 9. relative number of citations, 10. year
first published). These have been evaluated in a pairwise fashion to
produce Figure~\ref{fig:allfactors} A\&B, the R code for these is
given in the supplement.

The linear models that we used to test for relationships between
speed, accuracy and the above measures are:

\begin{equation*}
\begin{split}
accuracy=& c_0+c_1\times speed+c_2\times JIF+c_3\times H5+\\
& c_4\times citations+c_5\times Hindex+\\
& c_6\times Mindex+c_7\times relativeAge+\\
& c_8\times relativeCitations
\end{split}
\end{equation*}

\begin{equation*}
\begin{split}
speed=& c_0+c_{1}\times accuracy+c_{2}\times JIF+c_{3}\times H5+\\
& c_{4}\times citations+c_{5}\times Hindex+\\
& c_{6}\times Mindex+c_{7}\times relativeAge+\\
& c_{8}\times relativeCitations
\end{split}
\end{equation*}


For each benchmark of three or more tools, we extracted the published
accuracy and speed ranks. In order to identify if there is an
enrichment of certain accuracy and speed pairings we constructed a
permutation test. The individual accuracy and speed ranks were
reassigned to tools in a random fashion and each new accuracy and
speed rank pairing was recorded. For each benchmark this procedure was
repeated 10,000 times. These permuted rankings were normalised and
compared to the real rankings to produce the ‘x’ points in
Figure~\ref{fig:allfactors}B and the heatmap and histograms in
Figure~\ref{fig:speedaccuracy}. The heatmap in
Figure~\ref{fig:speedaccuracy} is based upon Z-scores
($Z=\frac{x-\bar{x}}{\sigma}$). For each cell in a $10\times 10$ grid
a Z-score is computed to illustrate the abundance or lack of tools in
a cell relative to the permuted data.

\section*{Data availability}
Raw datasets, software and documents are available under a CC-BY license:\\
\fussy
\url{https://docs.google.com/spreadsheets/d/14xIY2PHNvxmV9MQLpbzSfFkuy1RlzDHbBOCZLJKcGu8/edit?usp=sharing}\\
and here:\\
\fussy
\url{https://dx.doi.org/10.6084/m9.figshare.4299320.v1}\\
\sloppy
Additional documentation, code, figures and raw data is available here:\\
\fussy
\url{https://github.com/UCanCompBio/speed-vs-accuracy-meta-analysis}
\sloppy

\section*{Acknowledgements}

The authors acknowledge the valued contribution of invaluable
discussions with Anne-Laure Boulesteix, Shinichi Nakagawa, Suetonia
Palmer and Jason Tylianakis. Murray Cox, Raquel
Norel, Alexandros Stamatakis, Jens Stoye,Tandy Warnow, provided
valuable feedback on drafts of the manuscript.

PPG is supported by a Rutherford Discovery Fellowship,
administered by the Royal Society of New Zealand. 
{\bf AND ETC....}

%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\bibliographystyle{unsrt}
\bibliography{references.bib}





\end{document}




